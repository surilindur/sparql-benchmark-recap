# SPARQL Benchmark Recap

[![Build status](https://github.com/surilindur/sparql-benchmark-recap/workflows/CI/badge.svg)](https://github.com/surilindur/sparql-benchmark-recap/actions?query=workflow%3ACI)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Experimental scripts to analyse results from the [SPARQL benchmark runner](https://github.com/comunica/sparql-benchmark-runner.js),
as well the benchmarking dataset for [SolidBench](). Currently, the scripts allow calculating the following metrics:

    TODO

Furthermore, the following metrics can be plotted as graphs:

    TODO

## Installation

After cloning the repository, create a virtual environment and install the dependencies:

```bash
python -m venv .venv
source .venv/bin/activate
python -m pip install -r requirements.txt
```

## Usage

    TODO

## License

This code is released under the [MIT license](http://opensource.org/licenses/MIT).
